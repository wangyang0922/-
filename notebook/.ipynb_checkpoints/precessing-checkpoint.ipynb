{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append('/Users/roger/PycharmProjects/fe_nlp/project01/lecture01/code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2020-06-05 19:43:55,563 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/r9/q_nm2w7156bdy656wlps3vhh0000gn/T/jieba.cache\n",
      "2020-06-05 19:43:55,566 : DEBUG : Loading model from cache /var/folders/r9/q_nm2w7156bdy656wlps3vhh0000gn/T/jieba.cache\n",
      "Loading model cost 0.500 seconds.\n",
      "2020-06-05 19:43:56,064 : DEBUG : Loading model cost 0.500 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2020-06-05 19:43:56,065 : DEBUG : Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.data_loader import build_dataset,pad_proc,sentences_proc\n",
    "from utils.config import *\n",
    "from utils.multi_proc_utils import parallelize\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size 82943,test data size 20000\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "print('train data size {},test data size {}'.format(len(train_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 空值填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 空值剔除\n",
    "train_df.dropna(subset=['Report'], inplace=True)\n",
    "\n",
    "train_df.fillna('', inplace=True)\n",
    "test_df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.多线程, 批量数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df = parallelize(train_df, sentences_proc)\n",
    "test_df = parallelize(test_df, sentences_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 364 ms, sys: 329 ms, total: 693 ms\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = parallelize(train_df, sentences_proc)\n",
    "test_df = parallelize(test_df, sentences_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 合并训练测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size 82871,test data size 20000,merged_df data size 102871\n"
     ]
    }
   ],
   "source": [
    "train_df['merged'] = train_df[['Question', 'Dialogue', 'Report']].apply(lambda x: ' '.join(x), axis=1)\n",
    "test_df['merged'] = test_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x), axis=1)\n",
    "merged_df = pd.concat([train_df[['merged']], test_df[['merged']]], axis=0)\n",
    "print('train data size {},test data size {},merged_df data size {}'.format(len(train_df), len(test_df),len(merged_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 保存处理好的 训练 测试集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['merged'], axis=1)\n",
    "test_df = test_df.drop(['merged'], axis=1)\n",
    "train_df.to_csv(train_seg_path, index=None, header=True)\n",
    "test_df.to_csv(test_seg_path, index=None, header=True)\n",
    "merged_df.to_csv(merger_seg_path, index=None, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/roger/kaikeba/03_lecture/code/data/merged_train_test_seg_data.csv'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merger_seg_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:44:34,630 : INFO : collecting all words and their counts\n",
      "2019-11-23 21:44:34,631 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-23 21:44:34,843 : INFO : PROGRESS: at sentence #10000, processed 937272 words, keeping 36653 word types\n",
      "2019-11-23 21:44:35,055 : INFO : PROGRESS: at sentence #20000, processed 1889030 words, keeping 53934 word types\n",
      "2019-11-23 21:44:35,265 : INFO : PROGRESS: at sentence #30000, processed 2829438 words, keeping 66706 word types\n",
      "2019-11-23 21:44:35,468 : INFO : PROGRESS: at sentence #40000, processed 3741912 words, keeping 77607 word types\n",
      "2019-11-23 21:44:35,684 : INFO : PROGRESS: at sentence #50000, processed 4714603 words, keeping 87459 word types\n",
      "2019-11-23 21:44:35,913 : INFO : PROGRESS: at sentence #60000, processed 5748572 words, keeping 97387 word types\n",
      "2019-11-23 21:44:36,148 : INFO : PROGRESS: at sentence #70000, processed 6805872 words, keeping 106963 word types\n",
      "2019-11-23 21:44:36,361 : INFO : PROGRESS: at sentence #80000, processed 7748078 words, keeping 115067 word types\n",
      "2019-11-23 21:44:36,559 : INFO : PROGRESS: at sentence #90000, processed 8606035 words, keeping 122981 word types\n",
      "2019-11-23 21:44:36,755 : INFO : PROGRESS: at sentence #100000, processed 9455624 words, keeping 130011 word types\n",
      "2019-11-23 21:44:36,814 : INFO : collected 132022 word types from a corpus of 9704891 raw words and 102871 sentences\n",
      "2019-11-23 21:44:36,814 : INFO : Loading a fresh vocabulary\n",
      "2019-11-23 21:44:36,878 : INFO : effective_min_count=5 retains 32800 unique words (24% of original 132022, drops 99222)\n",
      "2019-11-23 21:44:36,879 : INFO : effective_min_count=5 leaves 9555789 word corpus (98% of original 9704891, drops 149102)\n",
      "2019-11-23 21:44:36,943 : INFO : deleting the raw counts dictionary of 132022 items\n",
      "2019-11-23 21:44:36,946 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2019-11-23 21:44:36,947 : INFO : downsampling leaves estimated 8595116 word corpus (89.9% of prior 9555789)\n",
      "2019-11-23 21:44:36,998 : INFO : estimated required memory for 32800 words and 300 dimensions: 95120000 bytes\n",
      "2019-11-23 21:44:36,998 : INFO : resetting layer weights\n",
      "2019-11-23 21:44:41,307 : INFO : training model with 8 workers on 32800 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2019-11-23 21:44:42,318 : INFO : EPOCH 1 - PROGRESS: at 19.80% examples, 1691172 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-23 21:44:43,319 : INFO : EPOCH 1 - PROGRESS: at 41.12% examples, 1740654 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-23 21:44:44,324 : INFO : EPOCH 1 - PROGRESS: at 60.49% examples, 1756504 words/s, in_qsize 14, out_qsize 2\n",
      "2019-11-23 21:44:45,326 : INFO : EPOCH 1 - PROGRESS: at 80.17% examples, 1757085 words/s, in_qsize 13, out_qsize 2\n",
      "2019-11-23 21:44:46,153 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-23 21:44:46,154 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-23 21:44:46,157 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-23 21:44:46,158 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-23 21:44:46,162 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-23 21:44:46,167 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 21:44:46,169 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 21:44:46,170 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 21:44:46,170 : INFO : EPOCH - 1 : training on 9704891 raw words (8595444 effective words) took 4.9s, 1768571 effective words/s\n",
      "2019-11-23 21:44:46,171 : INFO : training on a 9704891 raw words (8595444 effective words) took 4.9s, 1767319 effective words/s\n"
     ]
    }
   ],
   "source": [
    "wv_model = Word2Vec(LineSentence(merger_seg_path),\n",
    "                    size=300, \n",
    "                    negative=5, \n",
    "                    workers=8, i\n",
    "                    ter=wv_train_epochs, \n",
    "                    window=3,\n",
    "                    min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. 建立词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32800"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {word: index for index, word in enumerate(wv_model.wv.index2word)}\n",
    "reverse_vocab = {index: word for index, word in enumerate(wv_model.wv.index2word)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1 .使用了min_count,其实部分词不在vocab表中 ,但是训练数据和测试数据中又有这些词?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 获取词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32800, 300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = wv_model.wv.vectors\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 构建训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 可以把Question,Dialogue当做一句 `长文本处理`, 合并构建成X\n",
    "+ Report作为需要预测的标签,构建Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['X'] = train_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x), axis=1)\n",
    "test_df['X'] = test_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    方向机 重 助力 泵 方向机 都 换 新 都 换 助力 泵 方向机 换 方向机 带 助力 重...\n",
       "1    奔驰 ML500 排气 凸轮轴 调节 错误 有没有 电脑 检测 故障 代码 有发 一下 发动...\n",
       "2    2010 款 宝马X1 2011 年 出厂 20 排量 通用 6L45 变速箱 原地 换挡 ...\n",
       "3    30V6 发动机 号 位置 照片 最好 右侧 排气管 上方 缸体 上 靠近 变速箱 是不是 ...\n",
       "4    2012 款 奔驰 c180 维修保养 动力 值得 拥有 家庭 用车 入手 维修保养 费用 ...\n",
       "Name: X, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['X'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 2. 句子长度一样 ? 如何构建训练,batch操作,矩阵 ...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 填充字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_proc(sentence, max_len, vocab):\n",
    "    '''\n",
    "    < start > < end > < pad > < unk >\n",
    "    '''\n",
    "    # 0.按空格统计切分出词\n",
    "    words = sentence.strip().split(' ')\n",
    "    # 1. 截取规定长度的词数\n",
    "    words = words[:max_len]\n",
    "    # 2. 填充< unk > ,判断是否在vocab中, 不在填充 < unk >\n",
    "    sentence = [word if word in vocab else '<UNK>' for word in words]\n",
    "    # 3. 填充< start > < end >\n",
    "    sentence = ['<START>'] + sentence + ['<STOP>']\n",
    "    # 4. 判断长度，填充　< pad >\n",
    "    sentence = sentence + ['<PAD>'] * (max_len + 2 - len(words))\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. 如何确定max_len的值? 经验 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 获取适当的Max_Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(data):\n",
    "    \"\"\"\n",
    "    获得合适的最大长度值\n",
    "    :param data: 待统计的数据  train_df['Question']\n",
    "    :return: 最大长度值\n",
    "    \"\"\"\n",
    "    max_lens = data.apply(lambda x: x.count(' '))\n",
    "    return int(np.mean(max_lens) + 2 * np.std(max_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取输入数据 适当的最大长度\n",
    "train_y_max_len = get_max_len(train_df['X'])\n",
    "test_y_max_len = get_max_len(test_df['X'])\n",
    "y_max_len = max(train_y_max_len, test_y_max_len)\n",
    "\n",
    "# 获取标签数据 适当的最大长度\n",
    "train_y_max_len = get_max_len(train_df['Report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 填充处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ < start > - 句子开始\n",
    "+ < end > - 句子结尾\n",
    "+ < pad > - 短句填充\n",
    "+ < unk > - 未知词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集X处理\n",
    "train_df['X'] = train_df['X'].apply(lambda x: pad_proc(x, X_max_len, vocab))\n",
    "# 训练集Y处理\n",
    "train_df['Y'] = train_df['Report'].apply(lambda x: pad_proc(x, train_y_max_len, vocab))\n",
    "# 测试集X处理\n",
    "test_df['X'] = test_df['X'].apply(lambda x: pad_proc(x, X_max_len, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存中间结果数据\n",
    "train_df['X'].to_csv(train_x_pad_path, index=None, header=False)\n",
    "train_df['Y'].to_csv(train_y_pad_path, index=None, header=False)\n",
    "test_df['X'].to_csv(test_x_pad_path, index=None, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 新加的符号不在词表 和 词向量矩阵中,怎么办?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 词表更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 22:15:11,768 : INFO : collecting all words and their counts\n",
      "2019-11-23 22:15:11,769 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start retrain w2v model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 22:15:12,250 : INFO : PROGRESS: at sentence #10000, processed 2610000 words, keeping 22464 word types\n",
      "2019-11-23 22:15:12,716 : INFO : PROGRESS: at sentence #20000, processed 5220000 words, keeping 27515 word types\n",
      "2019-11-23 22:15:13,177 : INFO : PROGRESS: at sentence #30000, processed 7830000 words, keeping 29902 word types\n",
      "2019-11-23 22:15:13,649 : INFO : PROGRESS: at sentence #40000, processed 10440000 words, keeping 31156 word types\n",
      "2019-11-23 22:15:14,109 : INFO : PROGRESS: at sentence #50000, processed 13050000 words, keeping 31848 word types\n",
      "2019-11-23 22:15:14,585 : INFO : PROGRESS: at sentence #60000, processed 15660000 words, keeping 32301 word types\n",
      "2019-11-23 22:15:15,063 : INFO : PROGRESS: at sentence #70000, processed 18270000 words, keeping 32557 word types\n",
      "2019-11-23 22:15:15,534 : INFO : PROGRESS: at sentence #80000, processed 20880000 words, keeping 32657 word types\n",
      "2019-11-23 22:15:15,668 : INFO : collected 32683 word types from a corpus of 21629331 raw words and 82871 sentences\n",
      "2019-11-23 22:15:15,668 : INFO : Updating model with new vocabulary\n",
      "2019-11-23 22:15:15,685 : INFO : New added 27194 unique words (45% of original 59877) and increased the count of 27194 pre-existing words (45% of original 59877)\n",
      "2019-11-23 22:15:15,796 : INFO : deleting the raw counts dictionary of 32683 items\n",
      "2019-11-23 22:15:15,797 : INFO : sample=0.001 downsamples 18 most-common words\n",
      "2019-11-23 22:15:15,797 : INFO : downsampling leaves estimated 14298617 word corpus (66.2% of prior 21611599)\n",
      "2019-11-23 22:15:15,832 : INFO : estimated required memory for 54388 words and 300 dimensions: 157725200 bytes\n",
      "2019-11-23 22:15:15,832 : INFO : updating layer weights\n",
      "2019-11-23 22:15:15,844 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-11-23 22:15:15,844 : INFO : training model with 8 workers on 32804 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2019-11-23 22:15:16,853 : INFO : EPOCH 1 - PROGRESS: at 14.40% examples, 1004270 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-23 22:15:17,856 : INFO : EPOCH 1 - PROGRESS: at 28.34% examples, 1001052 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-23 22:15:18,866 : INFO : EPOCH 1 - PROGRESS: at 40.95% examples, 955266 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-23 22:15:19,874 : INFO : EPOCH 1 - PROGRESS: at 50.53% examples, 876160 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-23 22:15:20,880 : INFO : EPOCH 1 - PROGRESS: at 57.82% examples, 807049 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-23 22:15:21,890 : INFO : EPOCH 1 - PROGRESS: at 68.55% examples, 804561 words/s, in_qsize 13, out_qsize 2\n",
      "2019-11-23 22:15:22,893 : INFO : EPOCH 1 - PROGRESS: at 83.27% examples, 849946 words/s, in_qsize 15, out_qsize 1\n",
      "2019-11-23 22:15:23,895 : INFO : EPOCH 1 - PROGRESS: at 97.30% examples, 866874 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-23 22:15:24,072 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-23 22:15:24,076 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-23 22:15:24,077 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-23 22:15:24,080 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-23 22:15:24,081 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-23 22:15:24,081 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 22:15:24,083 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 22:15:24,085 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 22:15:24,085 : INFO : EPOCH - 1 : training on 21629331 raw words (7166547 effective words) took 8.2s, 870060 effective words/s\n",
      "2019-11-23 22:15:24,085 : INFO : training on a 21629331 raw words (7166547 effective words) took 8.2s, 869676 effective words/s\n",
      "2019-11-23 22:15:24,086 : INFO : collecting all words and their counts\n",
      "2019-11-23 22:15:24,087 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-23 22:15:24,172 : INFO : PROGRESS: at sentence #10000, processed 340000 words, keeping 7646 word types\n",
      "2019-11-23 22:15:24,254 : INFO : PROGRESS: at sentence #20000, processed 680000 words, keeping 10405 word types\n",
      "2019-11-23 22:15:24,334 : INFO : PROGRESS: at sentence #30000, processed 1020000 words, keeping 12278 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 22:15:24,413 : INFO : PROGRESS: at sentence #40000, processed 1360000 words, keeping 13807 word types\n",
      "2019-11-23 22:15:24,494 : INFO : PROGRESS: at sentence #50000, processed 1700000 words, keeping 14983 word types\n",
      "2019-11-23 22:15:24,575 : INFO : PROGRESS: at sentence #60000, processed 2040000 words, keeping 16099 word types\n",
      "2019-11-23 22:15:24,659 : INFO : PROGRESS: at sentence #70000, processed 2380000 words, keeping 17192 word types\n",
      "2019-11-23 22:15:24,738 : INFO : PROGRESS: at sentence #80000, processed 2720000 words, keeping 17951 word types\n",
      "2019-11-23 22:15:24,761 : INFO : collected 18179 word types from a corpus of 2817614 raw words and 82871 sentences\n",
      "2019-11-23 22:15:24,761 : INFO : Updating model with new vocabulary\n",
      "2019-11-23 22:15:24,769 : INFO : New added 7057 unique words (27% of original 25236) and increased the count of 7057 pre-existing words (27% of original 25236)\n",
      "2019-11-23 22:15:24,799 : INFO : deleting the raw counts dictionary of 18179 items\n",
      "2019-11-23 22:15:24,800 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2019-11-23 22:15:24,800 : INFO : downsampling leaves estimated 2140770 word corpus (76.5% of prior 2797425)\n",
      "2019-11-23 22:15:24,836 : INFO : estimated required memory for 14114 words and 300 dimensions: 40930600 bytes\n",
      "2019-11-23 22:15:24,836 : INFO : updating layer weights\n",
      "2019-11-23 22:15:24,847 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-11-23 22:15:24,847 : INFO : training model with 8 workers on 32804 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2019-11-23 22:15:25,851 : INFO : EPOCH 1 - PROGRESS: at 63.15% examples, 683961 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-23 22:15:26,258 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-23 22:15:26,261 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-23 22:15:26,265 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-23 22:15:26,272 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-23 22:15:26,273 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-23 22:15:26,274 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 22:15:26,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 22:15:26,278 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 22:15:26,278 : INFO : EPOCH - 1 : training on 2817614 raw words (1090835 effective words) took 1.4s, 764253 effective words/s\n",
      "2019-11-23 22:15:26,278 : INFO : training on a 2817614 raw words (1090835 effective words) took 1.4s, 762285 effective words/s\n",
      "2019-11-23 22:15:26,279 : INFO : collecting all words and their counts\n",
      "2019-11-23 22:15:26,280 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 22:15:26,776 : INFO : PROGRESS: at sentence #10000, processed 2610000 words, keeping 22625 word types\n",
      "2019-11-23 22:15:27,254 : INFO : collected 27243 word types from a corpus of 5220000 raw words and 20000 sentences\n",
      "2019-11-23 22:15:27,254 : INFO : Updating model with new vocabulary\n",
      "2019-11-23 22:15:27,267 : INFO : New added 12433 unique words (31% of original 39676) and increased the count of 12433 pre-existing words (31% of original 39676)\n",
      "2019-11-23 22:15:27,321 : INFO : deleting the raw counts dictionary of 27243 items\n",
      "2019-11-23 22:15:27,322 : INFO : sample=0.001 downsamples 18 most-common words\n",
      "2019-11-23 22:15:27,322 : INFO : downsampling leaves estimated 3406815 word corpus (65.7% of prior 5189026)\n",
      "2019-11-23 22:15:27,358 : INFO : estimated required memory for 24866 words and 300 dimensions: 72111400 bytes\n",
      "2019-11-23 22:15:27,358 : INFO : updating layer weights\n",
      "2019-11-23 22:15:27,369 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-11-23 22:15:27,369 : INFO : training model with 8 workers on 32804 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2019-11-23 22:15:28,373 : INFO : EPOCH 1 - PROGRESS: at 61.18% examples, 1068695 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-23 22:15:29,096 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-23 22:15:29,107 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-23 22:15:29,108 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-23 22:15:29,109 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-23 22:15:29,110 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-23 22:15:29,112 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-23 22:15:29,113 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-23 22:15:29,116 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-23 22:15:29,116 : INFO : EPOCH - 1 : training on 5220000 raw words (1735119 effective words) took 1.7s, 995092 effective words/s\n",
      "2019-11-23 22:15:29,116 : INFO : training on a 5220000 raw words (1735119 effective words) took 1.7s, 993197 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1735119, 5220000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('start retrain w2v model')\n",
    "wv_model.build_vocab(LineSentence(train_x_pad_path), update=True)\n",
    "wv_model.train(LineSentence(train_x_pad_path), epochs=wv_train_epochs, total_examples=wv_model.corpus_count)\n",
    "print('1/3')\n",
    "wv_model.build_vocab(LineSentence(train_y_pad_path), update=True)\n",
    "wv_model.train(LineSentence(train_y_pad_path), epochs=wv_train_epochs, total_examples=wv_model.corpus_count)\n",
    "print('2/3')\n",
    "wv_model.build_vocab(LineSentence(test_x_pad_path), update=True)\n",
    "wv_model.train(LineSentence(test_x_pad_path), epochs=wv_train_epochs, total_examples=wv_model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 22:16:53,722 : INFO : saving Word2Vec object under /home/roger/kaikeba/03_lecture/code/data/wv/word2vec.model, separately None\n",
      "2019-11-23 22:16:53,723 : INFO : not storing attribute vectors_norm\n",
      "2019-11-23 22:16:53,723 : INFO : not storing attribute cum_table\n",
      "2019-11-23 22:16:54,247 : INFO : saved /home/roger/kaikeba/03_lecture/code/data/wv/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "# 保存词向量模型\n",
    "wv_model.save(save_wv_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5.为什么不一开始就添加 标志符号,然后训练词向量?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32804, 300)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 更新vocab \n",
    "vocab = {word: index for index, word in enumerate(wv_model.wv.index2word)}\n",
    "reverse_vocab = {index: word for index, word in enumerate(wv_model.wv.index2word)}\n",
    "# 更新词向量矩阵\n",
    "embedding_matrix = wv_model.wv.vectors\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. 词可以训练吗?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <START> 方向机 重 助力 泵 方向机 都 换 新 都 换 助力 泵 方向机 换 方向...\n",
       "1    <START> 奔驰 <UNK> 排气 凸轮轴 调节 错误 有没有 电脑 检测 故障 代码 ...\n",
       "2    <START> 2010 款 宝马X1 2011 年 出厂 20 排量 通用 <UNK> 变...\n",
       "3    <START> 30V6 发动机 号 位置 照片 最好 右侧 排气管 上方 缸体 上 靠近 ...\n",
       "4    <START> 2012 款 奔驰 c180 维修保养 动力 值得 拥有 家庭 用车 入手 ...\n",
       "Name: X, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 数值转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遇到未知词就填充unk的索引\n",
    "unk_index = vocab['<UNK>']\n",
    "def transform_data(sentence,vocab):\n",
    "    # 字符串切分成词\n",
    "    words=sentence.split(' ')\n",
    "    # 按照vocab的index进行转换\n",
    "    ids=[vocab[word] if word in vocab else unk_index for word in words]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将词转换成索引  [<START> 方向机 重 ...] -> [32800, 403, 986, 246, 231\n",
    "train_ids_x=train_X.apply(lambda x:transform_data(x,vocab))\n",
    "train_ids_y=train_Y.apply(lambda x:transform_data(x,vocab))\n",
    "test_ids_x=test_X.apply(lambda x:transform_data(x,vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将索引列表转换成矩阵 [32800, 403, 986, 246, 231] --> array([[32800,   403,   986 ]]\n",
    "train_data_X=np.array(train_ids_x.tolist())\n",
    "train_data_Y=np.array(train_ids_y.tolist())\n",
    "test_data_X=np.array(test_ids_x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82871, 261)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 简易模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq(input_length, output_sequence_length, embedding_matrix, vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=300, weights=[embedding_matrix], trainable=False,\n",
    "                        input_length=input_length))\n",
    "    model.add(Bidirectional(GRU(300, return_sequences=False)))\n",
    "    model.add(Dense(300, activation=\"relu\"))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(Bidirectional(GRU(300, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(1e-3))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 基本参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82871, 261)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入的长度\n",
    "input_length = train_data_X.shape[1]\n",
    "# 输出的长度\n",
    "output_sequence_length = train_data_Y.shape[1]\n",
    "# 词表大小\n",
    "vocab_size=len(vocab)\n",
    "# 词向量矩阵\n",
    "embedding_matrix = wv_model.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 261, 300)          9841200   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 600)               1083600   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 34, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 34, 600)           1083600   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 34, 32804)         19715204  \n",
      "=================================================================\n",
      "Total params: 31,903,904\n",
      "Trainable params: 22,062,704\n",
      "Non-trainable params: 9,841,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = seq2seq(input_length,output_sequence_length,embedding_matrix,vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66296 samples, validate on 16575 samples\n",
      "15424/66296 [=====>........................] - ETA: 26:42 - loss: 3.4628"
     ]
    }
   ],
   "source": [
    "model.fit(train_data_X, train_data_Y, batch_size=32, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save('data/seq2seq_model.h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 .5 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_Y = model.predict(test_data_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 所有输出端，都以一个通用的<start>标记开头，以<end>标记结尾，这两个标记也视为一个词/字；\n",
    "\n",
    "2. 将<start>输入decoder，然后得到隐藏层向量，将这个向量与encoder的输出混合，然后送入一个分类器，分类器的结果应当输出P；\n",
    "\n",
    "3. 将P输入decoder，得到新的隐藏层向量，再次与encoder的输出混合，送入分类器，分类器应输出Q；\n",
    "\n",
    "4. 依此递归，直到分类器的结果输出<end>。\n",
    "    \n",
    "\n",
    "* 回到用seq2seq生成文章标题这个任务上，模型可以做些简化，并且可以引入一些先验知识。比如，由于输入语言和输出语言都是中文，因此encoder和decoder的Embedding层可以共享参数（也就是用同一套词向量）。这使得模型的参数量大幅度减少了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer # 词表构建 单词过滤 词频统计 序列填充\n",
    "from keras.preprocessing.sequence import pad_sequences # 序列数据填充\n",
    "from sklearn.model_selection import train_test_split # 数据集划分"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
